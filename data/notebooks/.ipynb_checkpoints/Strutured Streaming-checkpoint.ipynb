{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar uma aplicação em scala usando o spark para ler os dados da porta 9999 e exibir no console\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "# No terminal atualizar o net cat caso tenha feito um down\n",
    "# apt update\n",
    "# apt install netcat\n",
    "\n",
    "porta_leitura = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"9999\").load\n",
    "# antes de fazer o start executar o codigo abaixo no shell para realizar o streaming\n",
    "nc -lp 9999\n",
    "porta_saida = porta_leitura.writeStream.format(\"console\").start()\n",
    "\n",
    "# no jupyter \n",
    "# porta_saida = porta_leitura.writeStream.format(\"memory\").start() e pyconsole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 root supergroup       4551 2021-06-23 21:56 /user/marcos/data/exercises-data/iris/bezdekIris.data\r\n",
      "-rw-r--r--   3 root supergroup       4551 2021-06-23 21:56 /user/marcos/data/exercises-data/iris/iris.data\r\n"
     ]
    }
   ],
   "source": [
    "# 2. Ler os arquivos csv “hdfs://namenode:8020/user/<nome>/data/iris/*.data” em modo streaming com o seguinte schema:\n",
    "\n",
    "# Verificando o diretorio iris e o que tem dentro\n",
    "!hdfs dfs -ls /user/marcos/data/exercises-data/iris/*.data\n",
    "\n",
    "# bezdekIris.data 150 linhas\n",
    "# iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para trabalhar com o readStream precisando setar os schemas\n",
    "from pyspark.sql.types import StructType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_schema = StructType()\\\n",
    "    .add(\"sepal_lenght\",\"float\")\\\n",
    "    .add(\"sepal_width\",\"float\")\\\n",
    "    .add(\"petal_lenght\",\"float\")\\\n",
    "    .add(\"petal_width\",\"float\")\\\n",
    "    .add(\"class\",\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(sepal_lenght,FloatType,true),StructField(sepal_width,FloatType,true),StructField(petal_lenght,FloatType,true),StructField(petal_width,FloatType,true),StructField(class,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "# 3. Visualizar o schema das informações\n",
    "\n",
    "print(iris_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_lenght|sepal_width|petal_lenght|petal_width|      class|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Salvar os dados no diretório “hdfs://namenode:8020/user/<nome>/stream_iris/path” e o checkpoint em “hdfs://namenode:8020/user/<nome>/stream_iris/check”\n",
    "# Lendro o arquivo CSV e trazendo tudo que tem 'data' no comando *.data\n",
    "# inserindo o schema criado no data frame\n",
    "iris = spark.read.schema(iris_schema).csv(\"/user/marcos/data/exercises-data/iris/*.data\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_lenght: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_lenght: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = spark.read.schema(iris_schema).csv(\"/user/marcos/data/exercises-data/iris/*.data\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserindo o stream no DF READSTREAM\n",
    "iris = spark.readStream.schema(iris_schema).csv(\"/user/marcos/data/exercises-data/iris/*.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_lenght: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_lenght: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_saida = iris.writeStream.format(\"csv\")\\\n",
    "        .option(\"checkpointLocation\",\"/user/marcos/stream_iris/check\")\\\n",
    "        .option(\"path\",\"/user/marcos/stream_iris/path\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '58f55def-2f72-4f44-be8c-7c7c526b8905',\n",
       " 'runId': '0ce02212-736d-46aa-8309-124c1deb3435',\n",
       " 'name': None,\n",
       " 'timestamp': '2021-07-06T00:48:25.191Z',\n",
       " 'batchId': 1,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'getOffset': 16, 'triggerExecution': 16},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[hdfs://namenode:8020/user/marcos/data/exercises-data/iris/*.data]',\n",
       "   'startOffset': {'logOffset': 0},\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'FileSink[/user/marcos/stream_iris/path]'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Verificar a saida no hdfs e entender como os dados foram salvos\n",
    "# usando as funções do dataStream\n",
    "iris_saida.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_saida.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
